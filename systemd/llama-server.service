[Unit]
Description=llama.cpp Server (Local LLM Inference)
After=network.target
Wants=network-online.target

[Service]
Type=simple
# TODO: Customize user/group for your deployment
User=jerkytreats
Group=jerkytreats

# Paths
# TODO: Update path to match your deployment
WorkingDirectory=/home/jerkytreats/ai/llama.cpp

# Environment
Environment="LLAMA_HOST=127.0.0.1"
Environment="LLAMA_PORT=8080"
Environment="LLAMA_CTX_SIZE=4096"
Environment="LLAMA_GPU_LAYERS=99"
Environment="LLAMA_THREADS=8"

# Run the server
# TODO: Update path to match your deployment
ExecStart=/home/jerkytreats/ai/lmserver/scripts/run-llama-server.sh

# Restart policy
Restart=on-failure
RestartSec=10

# Resource limits (optional, adjust as needed)
# LimitNOFILE=65535
# MemoryMax=100G

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-server

[Install]
WantedBy=multi-user.target

